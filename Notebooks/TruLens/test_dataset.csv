ConversationId;Question;Answer;Context
3dcfa13d-6817-4471-9b2a-c2bb69907209;when were first defined relational databases;"E.F. Codd first defined the relational database model in a 1970 article for the ACM (Association for Computing Machinery) entitled ""A Relational Model of Data for Large Shared Data Banks"" [Relational Databases].";"[{""chunk"":""go.  Others, such as IMS\u2122 and IDMS/R\u2122 do not have this \nlimitation.  However, if the developer or DBA changes the structure of the trees or networks in these \nsystems, previously developed application programs tend to become logically impaired.  \n \nNetwork and hierarchical DBMSs were popular from the late 1960s through the 1970s and even into the \nearly 1980s.  IBM\u0027s IMS\u2122 DBMS was the most popular of the hierarchical databases and Cullinet\u2019s \nIDMS/R\u2122 the most popular of the network databases.  Although relational databases are more commonly \nused in systems written today, databases based on both the network and hierarchical model are still in use \ntoday, mostly in legacy systems running on large mainframe computers.  \n\nRelational Databases \n \nE.F. Codd [COD70] first defined the relational database model in a 1970 article for ACM (Association for \nComputing Machinery) entitled \u201CA Relational Model of Data for Large Shared Data Banks.\u201D  In this article, \nCodd detailed what he felt were deficiencies in the existing DBMSs adhering to the network and \nhierarchical data model.  Codd\u2019s relational model uses, as its basis, the mathematical relation; set theory \nand first order predicate logic.  Codd further refined the relational model in two more papers written in \n1985.  In these two papers, he spelled out twelve rules to which a DBMS must adhere in order to be \nrelational.  These twelve rules invariably address the deficiencies Codd found in the industries earlier \nattempts at developing commercial DBMSs.  Codd\u2019s rules are the information rule, the guaranteed access \nrule, the rule requiring systematic treatment of null values, the rule requiring the existence of a dynamic on-\nline catalog based on the relational model, the comprehensive data sub-language rule, the view updating \nrule, the high-level insert, update, and delete rule, the physical data independence rule, the logical data"",""title"":""0305038v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0305/0305038v1.pdf""},{""chunk"":""attempt to map \nobjects to relations.  Object-relational databases allow for the storage of complex objects.  Oracle 8i and \nOracle 9i are two examples of object-relational databases.  Oracle 8i and Oracle 9i both allow for the \nstorage of complex objects.  Oracle 9i allows for object inheritance.  Neither allows an object\u2019s behavior to \nbe stored along with its state.  [LOR99] [FRE02] \n \nOODBMSs generated significant interest, in both the research and business communities, in the early to the \nmiddle of the 1990s.  However, in recent years interest, from both communities, has waned.  Object-\nrelational databases now hold a significant portion of database market share, largely due to users upgrading \ntheir previously relational database to the vendor\u2019s newer object-relation database.  The most dominant of \nthese object-relational databases are Oracle 8i and Oracle 9i. \n\n 5\n\n\n\nIndustry Trends  \n\nIntroduction \n \nIt\u2019s human nature to always look for an easier and better way of doing things.  Computer Scientists are no \ndifferent from anyone else.  New technologies, in the field of Computer Science, will always be on the \nhorizon.  Still, the time it takes for a new technology to move from a new force to a dominant force is not a \nmatter of weeks.  Often, it is a matter of years or even decades. E.F. Codd published his first paper on \nRDBMSs in 1970.  He refined his definition of RDBMSs in 1985.  ORACLE\u00AE, according to their own web \nsite, released version 2 of Oracle in 1980 and version 5 in 1986.  ORACLE\u00AE did not begin to establish \nitself the dominant, commercially available, DBMS until the release of Oracle version 6 in 1988.  [ORA] \n \nIt takes time for a new technology to establish itself for a myriad of reasons.  Whenever a new technology \nappears there will always be those who embrace the new technology and those who reject the new \ntechnology.  There are advantages and disadvantages to both approaches to dealing with new technology."",""title"":""0305038v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0305/0305038v1.pdf""},{""chunk"":""preserve,\nand provide access to complex relational databases.\n\nIn this paper we present a method and applica-\ntion named \u201CSoftware-Invariant Archiving of Relational\nDatabases\u201D (SIARD), developed at the Swiss Federal\nArchives. It completely detaches typed relational data\nfrom almost any relational database management sys-\ntem, while still retaining most of the original data logic\nand integrating data and metadata in one archival infor-\nmation package that is based on text files and standard-\nized technologies. In Section II we discuss the technical\nand intellectual complexity of relational data in modern\ndatabase systems, the resulting problems for long-term\npreservation, and its relevance to archives. The objec-\ntives in the development of SIARD are described in Sec-\ntion III, while Section IV covers SIARD\u2019s system archi-\ntecture, workflow, features, and development platform.\n\nI. INTRODUCTION\n\nRelational data is one of the oldest forms of structured\ninformation representation, intuitively used already cen-\nturies before the \u201Cdigital age\u201D. With the rise of com-\nputer technology, the introduction of mathematical for-\nmulations of the relational data model in the mid-20th\n\nhttp://arXiv.org/abs/cs/0408054v1\n\n\n2\n\ncentury, and the international standardization of a corre-\nsponding data definition and query language, relational\ndata has become an omnipresent method to organize data\nfor electronic data processing in almost every field of\nwork, form business activities to scientific research and\ngovernment administration.\n\nDuring the past two decades, usage has developed from\nprocessing single-table data files with specific applica-\ntion software to generic relational database management\nsystems (RDBMS). These have internal mechanisms for\nlogical and physical organization of arbitrary relational\ndata models, are able to physically store terabytes of\ndata, cover rich data types (including internal procedural\ncode), enable multi-user transactions, and provide inter-"",""title"":""0408054v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0408/0408054v1.pdf""},{""chunk"":""research and\ngovernment administration.\n\nDuring the past two decades, usage has developed from\nprocessing single-table data files with specific applica-\ntion software to generic relational database management\nsystems (RDBMS). These have internal mechanisms for\nlogical and physical organization of arbitrary relational\ndata models, are able to physically store terabytes of\ndata, cover rich data types (including internal procedural\ncode), enable multi-user transactions, and provide inter-\nnal data life-cycle management. The definition, repre-\nsentation, management, and query of relational data was\nthereby standardized and separated from specific appli-\ncation logic and application software that operates on the\ndatabase. As a result, RDBMS have become core com-\nponents of almost any type of digital information system.\n\nIt is obvious that this development has decisive impacts\non the work of those institutions which are charged to\ncollect or accept digital data from various data sources,\nto make it broadly accessible, and to preserve it over\ndecades: national archives and libraries, science data\narchives, or business companies being under special legal\nregulations for long-term data retention (like, for exam-\nple, the pharmaceutical sector (19)).\n\nII. COMPLEXITY AND RELEVANCE\n\nA. Technical Complexity of Relational Data\n\nOne consequence of the progress in database technol-\nogy is that relational data and relational databases be-\ncome highly complex. They often consist of hundreds of\nlinked tables (i.e. physical representations of relational\nentities1), which makes it impossible to handle and query\ntable data outside an RDBMS if these links become bro-\nken or cannot be managed automatically anymore.\n\nFurthermore, any data item in a RDBMS has a pre-\ncisely defined data type and domain. (Entire tables or\nintegral parts of them may likewise have types.) Apart\nfrom basic data types (for example integer and real\nnumbers, dates, and character strings), low-level types"",""title"":""0408054v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0408/0408054v1.pdf""},{""chunk"":""and maintain the databases independent of any specific\nand short-lived products (or at least transferring them all\ninto only one preferred product). In fact, most RDBMS\nuse the same language for the definition of the internal\nlogical organization of data, namely the declarative (i.e.\nnon-procedural) Structured Query Language (SQL). De-\nspite of its name, the scope of SQL also includes the defi-\nnition of data structure and the manipulative operations\non data stored in that structure (17).\n\nThe development of SQL started in the 1970\u2019s, lead-\ning to the international standard ISO/IEC 9075 in 1987,\nand evolved in four main stages through SQL-89, SQL-\n92, SQL:1999, and recently SQL:2003 (20), while the size\nof the standard has grown from 120 to over 2\u2019000 pages.\nSQL:2003 and SQL:1999 are fully upward compatible\nwith SQL-92. The standard language keywords are struc-\ntured in three subsets: Data Definition (DDL), Data Ma-\nnipulation (DML), and Data Control and user authoriza-\ntion (DCL). To increase acceptance by vendors, the stan-\n\n\n\n3\n\ndard defines three levels of conformance and implemen-\ntation: entry, intermediate, and full level. The manda-\ntory part of SQL:1999 and later is called the \u201CCore\u201D of\nSQL and described in Part 2 (Foundation) and Part 11\n(Schemata) of the standard.\n\nAside from revisions to all parts of SQL:1999 (e.g. new\ndata types and functions that return entire tables (21)),\nthe 2003 edition contains the new part: \u201CSQL/XML\u201D de-\nfines a minimal handling and integration of text-based\ndata structured by the Extensible Markup Language\nXML (22). This includes (23) mappings between ta-\nbles and XML documents, SQL data types and XML\nSchema (24) data types, and RDBMS implementation-\nspecific character sets to Unicode (25; 26; 27) character\nencodings. Additionally, the related standard \u201CSQL Mul-\ntimedia and Application Packages \u201DISO/IEC 13249:2003\n(28) defines a number of packages of generic data types"",""title"":""0408054v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0408/0408054v1.pdf""},{""chunk"":""measured \nin six different ways (for a total of 60 \nattributes).  The image processing pipe-\nline also measures each galaxy\u2019s extent \nin several ways in each of the 5 color \nbands with error estimates. The pipeline \nassigns about a hundred additional \nproperties to each object \u2013 these attrib-\nutes are variously called flags, status, \nand type and are encoded as bit flags.    \n\nThe pipeline correlates each object with \nobjects in other catalogs: United States \nNaval Observatory [USNO], R\u00F6ntgen \nSatellite [ROSAT], Faint Images of the \nRadio Sky at Twenty-centimeters \n[FIRST], and others.   Successful corre-\nlations are recorded in a set of relation-\nship tables.    \n\nSpectrograms are the second kind of main data product pro-\nduced by the Sloan Digital Sky Survey.   About 600 spectra \nare observed at once using a single plate with optical fibers \ngoing to different CCDs.   The pipeline processing typically \nextracts about 30 spectral lines from each spectrogram and \ncarefully estimates the object\u2019s redshift. \n\nThe Relational Database Design \nOriginally, the SDSS developed the entire database on Objec-\n\ntivityDB\u2122 [SDSS-Science Ar-\nchive].  The designers used sub-\nclasses extensively:  for example \nthe PhotoObject has Star and \nGalaxy subclasses.  Objectiv-\nityDB supports arrays so the 5-\ncolors naturally mapped to vec-\ntors of 5 values. Connections to \nparents, children, spectra, and to \nother surveys were represented \nas object references.   Translating \nthe ObjectivityDB\u2122 design into \na relational schema was not \nstraightforward; but we wanted \nto preserve as much of the origi-\nnal design as possible in order to \npreserve the existing knowledge, \nskills, and documentation. \n\nThe SQL relational database language does not support point-\ners, arrays, or sub-classing \u2013 it is a much simpler data model.  \nThis is both a strength and a liability.  We approached the \nSQL database design by using views for subclassing, and by \nusing foreign keys for relationships."",""title"":""0111015v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0111/0111015v1.pdf""},{""chunk"":""attributes - right ascension and \ndeclination in the J2000 coordinate system, also represented as \nthe Cartesian components of a unit vector, and an index into a \nHierarchical Triangular Mesh (HTM).  They also have brightness \nstored in logarithmic units (magn itudes) with error bars in each \nof the five color bands.  These magnitudes are measured in six \ndifferent ways (for a total of 60 attributes).  The image process-\ning pipeline also measures each galaxy\u2019s extent in several ways \nin each of the 5 color bands with error estimates. The pipeline \nassigns about a hundred additional properties to each object \u2013 \nthese attributes are variously called flags, status, and type and are \nencoded as bit flags.   \n\nThe pipeline tries to correlate each object with objects in other \nsurveys: United States Naval Observatory [USNO], R\u00F6ntgen \nSatellite [ROSAT], Faint Images of the Radio Sky at Twenty-\ncentimeters [FIRST], and others.  Successful correlations are \nrecorded in a set of relationship tables.   \n\nSpectrograms are the other data product produced by the Sloan \nDigital Sky Survey.  About 600 spectra are observed at once \nusing a single plate with optical fibers going to different CCDs.  \nThe pipeline processing typically extracts about 30 spectral lines \nfrom each spectrogram and carefully estimates the object\u2019s red-\nshift. \n\n9.1. The Relational Database Design \nOriginally, the SDSS developed the entire database on Objectiv-\nityDB\u2122 [SDSS-Science Archive].  The designers used sub-\nclasses extensively:  for example the PhotoObject has Star and \nGalaxy subclasses.  ObjectivityDB support s arrays so the 5-\ncolors naturally mapped to vectors of 5 values. Connections to \nparents, children, spectra, and to other surveys were represented \nas object references.  Translating the ObjectivityDB\u2122 design \ninto a relational schema was not straightforward; but we wanted \nto preserve as much of the original design as possible in order to"",""title"":""0202013v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0202/0202013v1.pdf""},{""chunk"":""ar\nX\n\niv\n:c\n\ns/\n03\n\n06\n00\n\n3v\n1 \n\n [\ncs\n\n.D\nC\n\n] \n 3\n\n0 \nM\n\nay\n 2\n\n00\n3\n\nCHEP03, La Jolla, California, March 24-28, 2003 1\n\nR-GMA: First results after deployment\nAndrew Cooke and Werner Nutt\nHeriot-Watt, Edinburgh, UK\nJames Magowan, Manfred Oevers, and Paul Taylor\nIBM-UK\nAri Datta and Roney Cordenonsi\nQueen Mary, University of London, UK\nRob Byrom, Laurence Field, Steve Hicks, Manish Soni, Antony Wilson, and Xiaomei Zhu\nPPARC, UK\nLinda Cornwall, Abdeslem Djaoui, and Steve Fisher\nRutherford Appleton Laboratory, UK\nNorbert Podhorszki\nSZTAKI, Hungary\nBrian Coghlan, Stuart Kenny, David O\u2019Callaghan, and John Ryan\nTrinity College Dublin, Ireland\n\nWe describe R-GMA (Relational Grid Monitoring Architecture) which is being developed within the European\nDataGrid Project as an Grid Information and Monitoring System. Is is based on the GMA from GGF, which is\na simple Consumer-Producer model. The special strength of this implementation comes from the power of the\nrelational model. We offer a global view of the information as if each VO had one large relational database. We\nprovide a number of different Producer types with different characteristics; for example some support streaming\nof information. We also provide combined Consumer/Producers, which are able to combine information and\nrepublish it. At the heart of the system is the mediator, which for any query is able to find and connect to\nthe best Producers to do the job. We are able to invoke MDS info-provider scripts and publish the resulting\ninformation via R-GMA in addition to having some of our own sensors. APIs are available which allow the user\nto deploy monitoring and information services for any application that may be needed in the future. We have\nused it both for information about the grid (primarily to find what services are available at any one time) and\nfor application monitoring. R-GMA has been deployed in Grid testbeds, we describe the results and experiences\nof this deployment.\n\n1. Introduction"",""title"":""0306003v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0306/0306003v1.pdf""},{""chunk"":""semantics for peer consistent answers to first-order queries.\nFirst for the direct case, where transitive relationships between peers via ECs\n\nhttp://arxiv.org/abs/cs/0401015v1\n\n\nare not automatically considered; and at the end, the transitive case. We also\nillustrate by means of extended and representative examples, mechanisms for\nobtaining PCAs (a full treatment is left for an extended version of this paper).\nOne of the approaches is first order (FO) query rewriting, where the original\nquery is transformed into a new query, whose standard answers are the PCAs to\nthe original one. This methodology has intrinsic limitations. The second, more\ngeneral, approach is based on a specification of the solutions for a peer as the\nstable models of a logic program, which captures the different ways the system\nstabilizes after making the DECs and the trust relationships to be satisfied.\n\nWe first recall the definition of database repair that is used to characterize\nthe consistent answers to queries in single relational databases wrt certain in-\ntegrity constraints (ICs) [1]. Given a relational database instance r with schema\nR (which includes a domain D), \u03A3(r) is the set of ground atomic formulas\n{P (a\u0304) | P \u2208 R and r |= P (a\u0304)}.\n\nDefinition 1. [1] (a) Let r1, r2 be database instances over R. The distance,\n\u2206(r1, r2), between r1 and r2 is the symmetric difference \u2206(r1, r2) = (\u03A3(r1) \\\n\u03A3(r2)) \u222A (\u03A3(r2) \\\u03A3(r1)).\n(b) For database instances r, r1, r2, we define r1 \u2264r r2 if \u2206(r, r1) \u2286 \u2206(r, r2).\n(c) Let IC be a set of ICs on R. A repair of an instance r wrt IC is a \u2264r-minimal\ninstance r\u2032, such that r\u2032 |= IC . \uFFFD\n\nA repair of an instance r is a consistent instance that minimally differs from r.\n\n2 A Framework for P2P Data Exchange\n\nIn this section we will describe the framework we will use to formalize and\naddress the problem of query answering in P2P systems.\n\nDefinition 2. A P2P data exchange system P consists of:\n(a) A finite set P of peers, denoted by A, B, ..."",""title"":""0401015v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0401/0401015v1.pdf""},{""chunk"":""The Evolution of the \n\n \nComputerized Database \n\n \n \n\nNancy Hartline Bercich \n \n\nAdvisor: Dr. Don Goelman \n \n\nCompleted: Spring 2003 \n \n \n \n \n \n\n \n \n\n \nDepartment of Computing Sciences \n\n \nVillanova University \n\n \nVillanova, Pennsylvania \n\n \nU.S.A. \n\n \n\n\n\nDatabase History \n\nWhat is a database?  \n \nElsmari and Navathe [ELS00] define a database as a collection of related data.  By this definition, a \ndatabase can be anything from a homemaker\u2019s metal recipe file to a sophisticated data warehouse.  Of \ncourse, today when we think of databases we seldom think of a simple box of cards.  We invariably think \nof computerized data and their DBMS (database management systems). \n\nNon Computerized Databases  \n \nMy first experience with a serious database was in 1984.  Asplundh was installing Cullinet\u2019s IDMS/R\u2122 \n(Integrated Relational Data Management System) manufacturing system in the company\u2019s three \nmanufacturing facilities.  When first presented with the idea of a computerized database keeping track of \ninventory and the manufacturing process, the first plant manager\u2019s response was, \u201C Why do we need that?  \nWe already have a system.  We already have a method for keeping track of our inventory and \nmanufacturing process and we like our method.  Why change it?\u201D  He retrieved, from a metal filing cabinet, \nseveral boxes containing thousands of index cards. Each index card recorded data on a specific part, truck, \nmanufacturing process or supplier.  He already had a database, a good database that quite possibly predated \nthe modern computer.  In the end our task we easy.  We simply mechanized his existing database. \n \n\nFlat File Storage  \n \nBefore the advent of modern database technology computerized data was primarily stored in flat files of \nvarying formats, ISAM (Indexed Sequential Access Method) and VSAM (Virtual Storage Access Method) \nbeing two of the more common file formats of the time. Unfortunately, there are several disadvantages"",""title"":""0305038v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0305/0305038v1.pdf""}]"
3dcfa13d-6817-4471-9b2a-c2bb69907209;what is a stored procedure;"Stored Procedures are mentioned as programs stored inside the Relational Database Management System (RDBMS) to perform common or critical tasks which are not part of the specific application software outside the RDBMS. The idea behind using Stored Procedures is to keep a defined set of operations within the database environment, thereby allowing for consistent and efficient execution of these tasks directly on the data within the RDBMS[1].

In a more detailed context, Stored Procedures may include SQL code fragments along with procedural extensions from 4GL (Fourth Generation Language), and they are saved in database dictionary tables[2]. They can be invoked and executed as needed, and because they reside in the database, they are maintained alongside the data and metadata. This provides performance benefits by reducing the amount of data sent over the network, improving execution speed since the procedure is executed closer to the data, and enhancing maintainability by centralizing business logic within the database[3].

References:
1. [Complexity and Relevance](https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0408/0408054v1.pdf)
2. [Revolutions](https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0408/0408030v1.pdf)
3. [Principles](https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0111/0111015v1.pdf)";"[{""chunk"":""modern RDBMS include, for example:\n\n\u2022 Check constraints and assertions: For a single col-\numn of a table or a set of entire tables, assure that\nchanging or entering data does never violate de-\nfined data types, quantitative restrictions, or value\ndomains. In particular, it can be assured that data\nitems in a table column will never be empty.\n\n\u2022 Views: Assemble selected parts of several tables\nand operate on them as customized, virtual tables.\n\n\u2022 Triggers: Force the RDBMS to initiate timed op-\nerations on data when user-specified conditions are\nmet, for example log and audit user activities.\n\n\u2022 Functions (basic and user-defined): Perform nu-\nmerical calculations, conversions, or character op-\nerations on data items or sets.\n\n\u2022 Stored Procedures: Store and execute programs in-\nside the RDBMS to perform common or critical\ntasks which are not part of the specific application\nsoftware outside the RDBMS.\n\n\u2022 Foreign Keys: Ensure referential data integrity, i.e.,\nautomatically prevent that values can be stored in\nrows of one table if there are no corresponding val-\nues in referencing entries within the database.\n\n\u2022 Grants and Roles: Define user profiles and assign\nor withdraw privileges, for example to create new\ntables or access certain parts of the database.\n\nFor long-term preservation in national archives, rela-\ntional data is collected from many different database sys-\ntems and has to be retained and kept processable and\naccessible for decades. It is therefore essential to store\nand maintain the databases independent of any specific\nand short-lived products (or at least transferring them all\ninto only one preferred product). In fact, most RDBMS\nuse the same language for the definition of the internal\nlogical organization of data, namely the declarative (i.e.\nnon-procedural) Structured Query Language (SQL). De-\nspite of its name, the scope of SQL also includes the defi-\nnition of data structure and the manipulative operations\non data stored in that structure (17)."",""title"":""0408054v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0408/0408054v1.pdf""},{""chunk"":""principle, a query formulated on the\nlogical level must not only be insensitive to changes on the physical level, but\nalso to changes to the database schema, as long as these changes are to parts\nof the schema on which the query does not depend. To give a trivial example,\nthe SQL query select * from R where A\u003C5 still works if we drop from R\n\nsome column B different from A, but not if we drop column A itself. Turning\nthis around, it is thus useful to infer, given a query, under exactly which\nschemas it works, so that the programmer sees to which schema changes the\nquery is sensitive.\n\nSome recent trends in database systems seem to add weight to the above\nmotivation. Stored procedures [8] are 4GL and SQL code fragments stored\nin database dictionary tables. Whenever the schema changes, some of the\nstored procedures may become ill-typed, while others that were ill-typed may\nbecome well-typed. Knowing the principal type of each stored procedure\nmay be helpful in this regard. Models of semi-structured data [4, 3] loosen\n(or completely abandon) the assumption of a given fixed schema. Query\nlanguages for these models are essentially schema-independent. Nevertheless,\nas argued by Buneman et al. [2], querying is more effective if at least some\nform of schema is available, computed from the particular instance. Type\ninference can be helpful in telling for which schemas a given query is suitable.\n\nOhori, Buneman and Breazu-Tannen were probably the first to introduce\ntype inference in the context of database programming languages, in their\nwork on the language Machiavelli [11, 10]. Machiavelli features polymorphic\nfield selection from nested records, as well as a polymorphic join operator.\nHowever, the inference of principal types for full-fledged relational algebra\nexpressions was not taken up in that work. We should also mention the\nwork of Stemple et al. [14], who investigated reflective implementations of\nthe polymorphic relational algebra operators."",""title"":""0106035v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0106/0106035v1.pdf""},{""chunk"":""that are between 6,1,2,2 and 6,1,2,3.  So, when \nthe HTM IDs are mapped into a B-tree index they provide \nquick index for all the objects within a given triangle. The \nHTM library is an SQL extended stored procedure \nwrapped in a table-valued function \nspHTM_Cover(\u003Carea\u003E).   The \u003Carea\u003E can be either a circle \n(ra, dec, radius), a half-space (the intersection of planes), \n\nor a polygon defined by a sequence of points.  The function \nreturns a table, each row defining the start and end of an HTM \ntriangle.  One can join this table with the PhotoObj table to \nget a spatial subset of photo objects.  \n\nThe spHTM_Cover() function is too primitive for most users, \nthey actually want the objects nearby a certain object, or they \nwant all the objects in a certain area.   So, simpler functions \nare also supported.  For example: fGetNearestObjEq(1,1,1) \nreturns the nearest object within one arcminute of equatorial \ncoordinate (1\u00BA, 1\u00BA).  These procedures are frequently used in \nqueries and in the website access pages. \n\nSummary of Database Design \nIn summary, the logical database design consists of photo-\ngraphic and spectrographic objects.  They are organized into a \npair of snowflake schemas.   Subsetting views and many indi-\nces give convenient access to the conventional subsets (stars, \ngalaxies, ...).   Procedures and indices are defined to make \n\nspatial lookups convenient and fast. \n\nDatabase Physical Design  \nThe SkyServer initially took a simple ap -\nproach to database design \u2013 and since that \nworked, we stopped there.  The design \ncounts on the SQL storage engine and query \noptimizer to make all the intelligent \ndecisions about data layout and data access.  \n\nThe data tables are all created in one file \ngroup.  The database files are spread across \n4 mirrored volumes. Each of the 4 volumes \nholds a database file that starts at 20 GB and \n\nautomatically grows as needed.  The log files and temporary \ndatabase are also spread across these disks. SQL Server stripes"",""title"":""0111015v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0111/0111015v1.pdf""},{""chunk"":""introduction\nto materialized view maintenance using triggers can be found in [56].\n\n1.4.2 XML Views over Relational Databases\n\nIntegration of XML data with relational data is approached in literature\nfrom two directions. First, starting from XML data, it is stored in a rela-\ntional database to reuse the relational database\u2019s functionality in building a\ntransactional persistent XML database. Here, it has to be decided on how\nthe XML data is stored and indexed and how XML queries on the loaded\n\n8Select-project-join queries, an algebra without operators such as \u222A, \u2212, and \u03C1.\n\n\n\n24 1. Introduction\n\n(virtual) XML data are translated to queries on the relational data (e.g.,\nsee [74]). Obviously, the storage model influences query translation and ex-\necution. Second, starting from relational data, it is published as XML data\nto make it available on the Web or to facilitate data integration with other\nsources. Here, the focus lies on designing the mapping from relational to\nXML data and on how to efficiently generate XML from arbitrary relational\nschemas. This is different from the first area where the mapping is often\nfixed, however, the two areas are not distinct in the issues they address. For\nan overview of integrating XML with relational data, e.g., see [91].\n\nClearly, the area of publishing relational data as XML is related to the\nthesis\u2019 aim of maintaining consistency between relational data and Web\npages. Several approaches have been proposed in literature in this area,\nthe most prominent ones being SilkRoute [59] and XTABLES [65], which\nwas formerly known as XPERANTO [37]. SilkRoute and XPERANTO use\na query language to define views, while recent approaches also use differ-\nent techniques, i.e., [18] uses attribute translation grammars and [145] uses\nview correspondence assertions. As of today, commercial database products\nof major vendors IBM, Microsoft, and Oracle support defining restricted\nXML views over the database (see [65, 91] for an overview)."",""title"":""0501042v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0501/0501042v1.pdf""},{""chunk"":""introduction\nto materialized view maintenance using triggers can be found in [56].\n\n1.4.2 XML Views over Relational Databases\n\nIntegration of XML data with relational data is approached in literature\nfrom two directions. First, starting from XML data, it is stored in a rela-\ntional database to reuse the relational database\u2019s functionality in building a\ntransactional persistent XML database. Here, it has to be decided on how\nthe XML data is stored and indexed and how XML queries on the loaded\n\n8Select-project-join queries, an algebra without operators such as \u222A, \u2212, and \u03C1.\n\n\n\n24 1. Introduction\n\n(virtual) XML data are translated to queries on the relational data (e.g.,\nsee [74]). Obviously, the storage model influences query translation and ex-\necution. Second, starting from relational data, it is published as XML data\nto make it available on the Web or to facilitate data integration with other\nsources. Here, the focus lies on designing the mapping from relational to\nXML data and on how to efficiently generate XML from arbitrary relational\nschemas. This is different from the first area where the mapping is often\nfixed, however, the two areas are not distinct in the issues they address. For\nan overview of integrating XML with relational data, e.g., see [91].\n\nClearly, the area of publishing relational data as XML is related to the\nthesis\u2019 aim of maintaining consistency between relational data and Web\npages. Several approaches have been proposed in literature in this area,\nthe most prominent ones being SilkRoute [59] and XTABLES [65], which\nwas formerly known as XPERANTO [37]. SilkRoute and XPERANTO use\na query language to define views, while recent approaches also use differ-\nent techniques, i.e., [18] uses attribute translation grammars and [145] uses\nview correspondence assertions. As of today, commercial database products\nof major vendors IBM, Microsoft, and Oracle support defining restricted\nXML views over the database (see [65, 91] for an overview)."",""title"":""0501042v3.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0501/0501042v3.pdf""},{""chunk"":""behavior, as for example in [44], where triggers are derived\nfrom SQL statements to maintain materialized views. A brief introduction\nto materialized view maintenance using triggers can be found in [56].\n\n8Select-project-join queries, an algebra without operators such as \u222A, \u2212, and \u03C1.\n\n\n\n24 1. Introduction\n\n1.4.2 XML Views over Relational Databases\n\nIntegration of XML data with relational data is approached in literature\nfrom two directions. First, starting from XML data, it is stored in a rela-\ntional database to reuse the relational database\u2019s functionality in building a\ntransactional persistent XML database. Here, it has to be decided on how\nthe XML data is stored and indexed and how XML queries on the loaded\n(virtual) XML data are translated to queries on the relational data (e.g.,\nsee [74]). Obviously, the storage model influences query translation and ex-\necution. Second, starting from relational data, it is published as XML data\nto make it available on the Web or to facilitate data integration with other\nsources. Here, the focus lies on designing the mapping from relational to\nXML data and on how to efficiently generate XML from arbitrary relational\nschemas. This is different from the first area where the mapping is often\nfixed, however, the two areas are not distinct in the issues they address. For\nan overview of integrating XML with relational data, e.g., see [91].\n\nClearly, the area of publishing relational data as XML is related to the\nthesis\u2019 aim of maintaining consistency between relational data and Web\npages. Several approaches have been proposed in literature in this area,\nthe most prominent ones being SilkRoute [59] and XTABLES [65], which\nwas formerly known as XPERANTO [37]. SilkRoute and XPERANTO use\na query language to define views, while recent approaches also use differ-\nent techniques, i.e., [18] uses attribute translation grammars and [145] uses\nview correspondence assertions. As of today, commercial database products"",""title"":""0501042v2.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0501/0501042v2.pdf""},{""chunk"":""research and\ngovernment administration.\n\nDuring the past two decades, usage has developed from\nprocessing single-table data files with specific applica-\ntion software to generic relational database management\nsystems (RDBMS). These have internal mechanisms for\nlogical and physical organization of arbitrary relational\ndata models, are able to physically store terabytes of\ndata, cover rich data types (including internal procedural\ncode), enable multi-user transactions, and provide inter-\nnal data life-cycle management. The definition, repre-\nsentation, management, and query of relational data was\nthereby standardized and separated from specific appli-\ncation logic and application software that operates on the\ndatabase. As a result, RDBMS have become core com-\nponents of almost any type of digital information system.\n\nIt is obvious that this development has decisive impacts\non the work of those institutions which are charged to\ncollect or accept digital data from various data sources,\nto make it broadly accessible, and to preserve it over\ndecades: national archives and libraries, science data\narchives, or business companies being under special legal\nregulations for long-term data retention (like, for exam-\nple, the pharmaceutical sector (19)).\n\nII. COMPLEXITY AND RELEVANCE\n\nA. Technical Complexity of Relational Data\n\nOne consequence of the progress in database technol-\nogy is that relational data and relational databases be-\ncome highly complex. They often consist of hundreds of\nlinked tables (i.e. physical representations of relational\nentities1), which makes it impossible to handle and query\ntable data outside an RDBMS if these links become bro-\nken or cannot be managed automatically anymore.\n\nFurthermore, any data item in a RDBMS has a pre-\ncisely defined data type and domain. (Entire tables or\nintegral parts of them may likewise have types.) Apart\nfrom basic data types (for example integer and real\nnumbers, dates, and character strings), low-level types"",""title"":""0408054v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0408/0408054v1.pdf""},{""chunk"":""the agenda was \u201Cimplement SQL better.\u201D Now data-\nbase architectures are in the punctuated stage of punctuated-\nequilibrium.  They have become the vehicles to deliver an inte-\ngrated application development environment, to be data-rich \nnodes of the Internet, to do data discovery, and to be self-\nmanaging.  They are also our main hope to deal with the informa-\ntion avalanche hitting individuals, organizations, and all aspects \nof human organization.   It is an exciting time!  There are many \nexciting new research problems and many challenging implemen-\ntation problems. This talk highlights some of them.  \n\n2. THE REVOLUTIONS \n2.1 Object Relational Arrives \nWe be data.  But, you cannot separate data and algorithms.  Un-\nfortunately, Cobol has a data division and a procedure division \nand so it had separate committees to define each one.  The data-\nbase community inherited that artificial division from the Cobol \nData Base Task Group (DBTG). We were separated from our \nprocedural twin at birth.  We have been trying to reunite with it \nfor 40 years now.  In the mid-eighties stored procedures were \nadded to SQL (thank you Sybase), and there was a proliferation of \nobject-relational database systems.  In the mid-nineties many SQL \nvendors added objects to their own systems.  Although these were \neach good efforts, they were fundamentally flawed because de \nnovo language designs are very high risk.   \n\nThe object-oriented language community has been refining its \nideas since Simula67.  There are now several good OO languages \nwith excellent implementations and development environments \n(Java and C# for example.) There is a common language runtime \nthat supports nearly all languages with good performance. \n\nThe big news now is the marriage of databases and these lan-\nguages.  The runtimes are being added to the database engine so \nthat now one can write database stored-procedures (modules) in \nthese languages and can define database objects as classes in these"",""title"":""0408030v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0408/0408030v1.pdf""},{""chunk"":""preserve,\nand provide access to complex relational databases.\n\nIn this paper we present a method and applica-\ntion named \u201CSoftware-Invariant Archiving of Relational\nDatabases\u201D (SIARD), developed at the Swiss Federal\nArchives. It completely detaches typed relational data\nfrom almost any relational database management sys-\ntem, while still retaining most of the original data logic\nand integrating data and metadata in one archival infor-\nmation package that is based on text files and standard-\nized technologies. In Section II we discuss the technical\nand intellectual complexity of relational data in modern\ndatabase systems, the resulting problems for long-term\npreservation, and its relevance to archives. The objec-\ntives in the development of SIARD are described in Sec-\ntion III, while Section IV covers SIARD\u2019s system archi-\ntecture, workflow, features, and development platform.\n\nI. INTRODUCTION\n\nRelational data is one of the oldest forms of structured\ninformation representation, intuitively used already cen-\nturies before the \u201Cdigital age\u201D. With the rise of com-\nputer technology, the introduction of mathematical for-\nmulations of the relational data model in the mid-20th\n\nhttp://arXiv.org/abs/cs/0408054v1\n\n\n2\n\ncentury, and the international standardization of a corre-\nsponding data definition and query language, relational\ndata has become an omnipresent method to organize data\nfor electronic data processing in almost every field of\nwork, form business activities to scientific research and\ngovernment administration.\n\nDuring the past two decades, usage has developed from\nprocessing single-table data files with specific applica-\ntion software to generic relational database management\nsystems (RDBMS). These have internal mechanisms for\nlogical and physical organization of arbitrary relational\ndata models, are able to physically store terabytes of\ndata, cover rich data types (including internal procedural\ncode), enable multi-user transactions, and provide inter-"",""title"":""0408054v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0408/0408054v1.pdf""},{""chunk"":""rapid data analysis the fusion \ncommunity adopted a common data acquisition and management system, MDSplus [9] and a \ncommon relational database run-management schema, currently implemented in a Microsoft SQL \nServer. MDSplus was designed primarily to serve as a high-speed, hierarchical data acquisition \nsystem for fusion experiments. In addition to capabilities allowing it to accommodate both simple \nand complex data types, it also contains functionality enabling scientists to describe data \nacquisition and analysis tasks that need to be accomplished during an experimental pulse cycle, \nand dispatching these tasks to distributed servers. For example, a user can establish complex \nsequencing of such tasks by describing dependencies between them (e.g., execute task C only if \ntasks A and B have completed successfully). MDSplus also provides a mechanism for defining \nevents that should be declared when a particular task completes. Applications can notice the \noccurrence of these events and respond with some operation such as refresh a graphical display \nusing the newest data. \n \nAlthough MDSplus data servers provide efficient mechanisms for data storage and retrieval, they \nare difficult and inefficient to search by content. For this reason, their functionality was \nsupplemented by calls to the relational database providing a fast search capability. The relational \ndatabase stores metadata information about the data produced by a program as well as \ninformation on how to find that data in the MDSplus server. The metadata contains information \nabout who produced the data, what the data represents, and how and when it has been produced, \nand can optionally be augmented by highlights of the data. To retrieve a specific piece of data the \nscientist first searches the relational database, and having found information about the data he or \nshe needs, can take advantage of the fast retrieval option provided by MDSplus data server \nassociated with this database."",""title"":""0301033v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0301/0301033v1.pdf""}]"
3dcfa13d-6817-4471-9b2a-c2bb69907209;Does chloroquine really works against covid;I don't have enough information to respond to your question.;[]
3dcfa13d-6817-4471-9b2a-c2bb69907209;What is CLP;"Based on the information in the provided documents, the abbreviation CLP stands for ""Constraint Logic Programming."" The name CLP was first introduced by Jaffar and Lassez in 1986 for a general framework of a logic programming language that is parameterized with respect to constraint language and a domain of computation, yielding soundness and completeness results for an operational semantics relying on a constraint solver for the employed constraint language[1].

References:
[1] Introduction of the CLP abbreviation, [0008036v1.pdf](https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0008/0008036v1.pdf)";"[{""chunk"":""of conventional logic programming\n\n(Lloyd 1987), and involves the incorporation of constraint languages and constraint solving\n\nmethods into logic programming languages. The name CLP was first introduced by Jaffar and\n\nLassez (1986) for a general framework of a logic programming language that is parametrized\n\nwith respect to constraint language and a domain of computation, and yields soundness\n\nand completeness results for an operational semantics relying on a constraint solver for the\n\nemployed constraint language. For example, conventional logic programming or Prolog is ob-\n\ntained from CLP by employing equations between first order terms as constraint language\n\nand by interpreting these equations in the Herbrand universe. In this case the operational\n\nsemantics of SLD-resolution can be seen to rely on a constraint solver which solves term\n\nequations in the Herbrand universe by term unification. Recent extensions, refinements, and\n\nvarious applications of CLP are discussed in Jaffar and Maher (1994). In the following we will\n\nrely on the general CLP scheme of Ho\u0308hfeld and Smolka (1988), which has been shown to be a\n\nuseful tool for our intended application of linguistic knowledge representation (see Do\u0308rre and\n\n9\n\n\n\n10 Chapter 2. Foundations\n\nDorna (1993), Go\u0308tz (1995), Go\u0308tz and Meurers (1995)).\n\nThe term constraint logic grammars expresses the connection between CLP and constraint-\n\nbased grammars. That is, CLGs are understood as grammars formulated by means of a suit-\n\nable logical language which can be used as a constraint language in the CLP scheme of Ho\u0308hfeld\n\nand Smolka (1988). The idea behind this connection is to provide an operational treatment\n\nof purely declaratively specified grammars. This needs further explanation: Constraint-based\n\ngrammars enable a clear model-theoretic characterization of linguistic objects by specifying\n\ngrammars as sets of descriptions from a suitable logical description language, called the con-\n\nstraint language."",""title"":""0008036v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0008/0008036v1.pdf""},{""chunk"":""Clearly, in this setting\n\na model-theoretic semantics for probabilistic inference is superfluous since the values of the\n\nprobabilistic parameters are obtained by automatic statistical methods which are not ma-\n\nnipulable by the user. Rather, we are interested in a stochastic semantics for CLP inference\n\nwhich is determined by the log-linear probability model together with the statistical methods\n\nfor parameter estimation and property selection from given input data.\n\n1.5 Bibliographical Note\n\nVarious parts of this thesis are based upon previously published work of the author. Chap. 3 is\n\nan extended version of Riezler (1996). Chap. 4 is based upon work presented in Riezler (1997),\n\n\n\n1.4 Bibliographical Note 7\n\nRiezler (1998a), Riezler (1998b), and Johnson, Geman, Canon, Chi, and Riezler (1999).\n\n\n\n8 Chapter 1. Introduction\n\n\n\nChapter 2\n\nFoundations: Basic Concepts of\n\nCLP and CLGs\n\nIn this chapter we report the central formal concepts of the CLP scheme of Ho\u0308hfeld and\n\nSmolka (1988). In preparation for the following work we give some proofs missing in the\n\noriginal paper and present the CLP scheme in a slightly modified fashion. Furthermore, in\n\norder to prepare the running example of the next chapters, we report the main concepts of a\n\nfeature-based constraint language for HPSG and show how to embed this constraint language\n\ninto the CLP scheme, yielding feature-based CLGs.\n\n2.1 Introduction and Overview\n\nConstraint logic programming is a powerful extension of conventional logic programming\n\n(Lloyd 1987), and involves the incorporation of constraint languages and constraint solving\n\nmethods into logic programming languages. The name CLP was first introduced by Jaffar and\n\nLassez (1986) for a general framework of a logic programming language that is parametrized\n\nwith respect to constraint language and a domain of computation, and yields soundness\n\nand completeness results for an operational semantics relying on a constraint solver for the"",""title"":""0008036v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0008/0008036v1.pdf""},{""chunk"":""final chapter we present a short summary of the work of this thesis. We compare the ad-\n\nvantages and shortcomings of the two presented approaches to quantitative and probabliistic\n\nCLP relative to each other and relative to other approaches. Not surprisingly, the presented\n\nwork is not definitive but raises several questions which could not be answered in the course\n\nof this thesis. These questions will be dealt with when we discuss future continuations of the\n\npresented work.\n\n5.1 Summary\n\nIn this thesis, we have presented new mathematical and algorithmic techniques for quantita-\n\ntive and statistical inference in constraint-based NLP. We have chosen the general concepts of\n\nCLP as the formal framework to deal with constraint-based NLP, yielding CLGs as instances\n\nof CLP. Aiming at a general solution of the problem of structural ambiguity in CLGs, we\n\nhave presented two independent approaches to weighted CLGs.\n\nThe first approach, called quantitative CLP, is situated in a clear logical framework, and\n\npresents a sound and complete system of quantitative inference for definite clauses with sub-\n\njective weights attached to them. This approach permits to specify weights in arbitrary ways,\n\ne.g., as subjective probabilities, user-defined preference values, or degrees of grammaticality,\n\nand to use search techniques such as alpha-beta pruning for finding the maximally weighted\n\nproof tree for a given set of queries efficiently. Related previous work either focussed solely\n\non formal semantics of quantitative logic programs without specific applications in mind,\n\nor presented only informal attachments of weights to grammar components for the aim of\n\nweight-based pruning in natural language parsing. Our approach is the first one to combine\n\nweight-based parsing for constraint-based systems with a rigid formal semantics for such\n\nquantitative inference systems.\n\n129\n\n\n\n130 Chapter 5. Conclusion"",""title"":""0008036v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0008/0008036v1.pdf""},{""chunk"":""deterministi\r programs\n\nand thus, obtaining smaller programs.\n\nWhen the unfold/fold transformation strategy is used for program synthesis,\n\nit is often the \rase that the above mentioned transformations also improve the\n\ne\uFFFD\rien\ry of the derived programs.\n\nFinally, we would like to noti\re that the unfold/fold transformation strategy\n\n\ran be applied starting from a program P \u222A Cls(f, \u03D5\u03C4 ) (instead of NatSet \u222A\nCls(f, \u03D5\u03C4 )) where: (i) P is the output of a previous appli\ration of the strategy,\n\nand (ii) \u03D5 is a formula built like a WS1S formula, ex\rept that it uses predi-\n\n\rates o\r\rurring in P (besides \u2264 and \u2208). Thus, we \ran synthesize programs (or\n\n\ronstru\rt proofs) in a \rompositional way, by \uFFFDrst synthesizing programs for sub-\n\nformulas. We will follow this \rompositional methodology in the example of the\n\nfollowing Se\rtion 7.\n\n7 An Appli\ration to the Veri\uFFFD\ration of In\uFFFDnite State\n\nSystems: the Dynami\r Bakery Proto\rol\n\nIn this se\rtion we present an example of veri\uFFFD\ration of a safety property of\n\nan in\uFFFDnite state system by \ronsidering CLP(WS1S) programs [?\u2104. As already\n\nmentioned, by applying our unfold/fold synthesis method we will then translate\n\nCLP(WS1S) programs into logi\r programs.\n\nThe syntax of CLP(WS1S) programs is de\uFFFDned as follows. We \ronsider a set of\n\nuser-de\uFFFDned predi\rate symbols. A CLP(WS1S) \rlause is of the form A\u2190 \u03D5\u2227G,\nwhere A is an atom, \u03D5 is a formula of WS1S, G is a goal, and the predi\rates\n\no\r\rurring in A or in G are all user-de\uFFFDned. A CLP(WS1S) program is a set of\n\nCLP(WS1S) \rlauses. We assume that CLP(WS1S) programs are strati\uFFFDed.\n\nGiven a CLP(WS1S) program P , we de\uFFFDne the semanti\rs of P to be its\n\nperfe\rt model, denoted M(P ) (here we extend to CLP(WS1S) programs the\n\nde\uFFFDnitions whi\rh are given for normal logi\r programs in [?\u2104).\n\nOur example \ron\rerns the Dynami\r Bakery proto\rol, \ralled DBakery for\n\nshort, and we prove that it ensures mutual ex\rlusion in a system of pro\resses\n\nwhi\rh share a \rommon resour\re, even if the number of pro"",""title"":""0311043v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0311/0311043v1.pdf""},{""chunk"":""and other Prolog programs.\n\n1 Introduction\n\nThe class CLP(X ) of Constraint Logic Programming languages was introduced by\n\nJaffar and Lassez (Jaffar \u0026 Lassez, 1987) as a generalization of the innovative fea-\n\ntures introduced by Colmerauer in Prolog II (Colmerauer, 1984; Colmerauer, 1985):\n\nnamely computing in Prolog with other structures than the Herbrand terms, with\n\ninequality constraints and with co-routining.\n\nInherited from the Prolog tradition, CLP(X ) programs are untyped. Usually the\n\nstructure of interest X is however a quite complex combination of basic struc-\n\ntures that may include integer arithmetic, real arithmetic, booleans, lists, Her-\n\nbrand terms, infinite terms, etc. with implicit coercions between constraint do-\n\nmains like in Prolog IV (Colmerauer, 1996). Even the early CLP(R) system of\n\n(Jaffar \u0026 Lassez, 1987) already combines Herbrand terms with arithmetic expres-\n\nsions in a non-symmetrical way: any arithmetic expression may appear under a Her-\n\nbrand function symbol, e.g. in a list, but not the other way around. The framework\n\nof many sorted logic in (Jaffar \u0026 Lassez, 1987) is not adequate for representing the\n\ntype system underlying such a combination, as it forces Herbrand function symbols\n\nto have a unique type (e.g. over reals or Herbrand terms), whereas Herbrand func-\n\ntions can be used polymorphically, e.g. in f(1) and f(f(1)), or the list constructor\n\nin a list of list of numbers [[3]].\n\nhttp://arxiv.org/abs/cs/0405100v1\n\n\n2 Franc\u0327ois Fages and Emmanuel Coquery\n\nThe type system of Mycroft-O\u2019Keefe (Mycroft \u0026 O\u2019Keefe, 1984; Lakshman \u0026 Reddy, 1991;\n\nHill \u0026 Topor, 1992) is an adaptation to logic programming of the first type system\n\nwith parametric polymorphism, that was introduced by Damas-Milner for the func-\n\ntional programming language ML. In this system, types are first-order terms, type\n\nvariables inside types, like \u03B1 in list(\u03B1), express type parameters. Programs defined"",""title"":""0405100v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0405/0405100v1.pdf""},{""chunk"":""abstract operational\nsemantics is sufficient. The information given in an abstract operational semantics is correct but not\ncomplete. It tells tracer developers what information should be provided to users and it tells users\nhow to interpret this trace information. In the case of CLP(FD) there was no operational semantics\nthat we could use to specify a tracer and we designed an abstract operational semantics as a set of\nstate transition rules. .\n\n2.3 Informal presentation of domain reduction\n\nBefore we give examples of a formal specification of events we have to informally explain how vari-\nable domains are reduced. This is an essential mechanism of constraint propagation in the case of\nfinite domains. A CLP(FD) program searches a solution for a set of variables which take values\nover finite domains and which must verify a set of constraints. The evolution of the domains can\nbe viewed as a sequence of applications of reduction operators attached to the constraints. Each op-\nerator can be applied several times until the computation reaches a fix-point [FLT00]. This fix-point\nis the set of final domain states. An example of computation with reduction operators is shown in\nFigure 1. There are three variables x, y and z and two constraints, x \u003E y and y \u003E z. A set of possi-\nble values is associated to each variable. This set is called the domain of the variable. The domain\nreduction consists in elementary steps that remove inconsistent values from those domains. At the\nbeginning, the domain of x, Dx, the domain of y, Dy , and the domain of z, Dz , are all equal to {1, 2, 3}.\nThis is represented by three columns of white squares. Considering the first constraint, it appears that\nx cannot take the value \u201C1\u201D, because otherwise there would be no value for y such that x \u003E y; this in-\nconsistent value is withdrawn from Dx. This withdrawal is marked with a black square. In the same\nway, the value 3 is withdrawn from the domain of y."",""title"":""0310042v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0310/0310042v1.pdf""},{""chunk"":""so if KAC does not detect inconsistency, AC\n\nwill not detect inconsistency either.\n\nKAC is equivalent to AC when domains are completely known. The advantage\n\nin using KAC is that the check for known arc-consistency can be performed lazily,\n\nwithout full knowledge of all the elements in every domain.\n\n2.3 Linking the two sorts\n\nIntuitively, we want to bind CLP(FD) with CLP(I-Set) with the intended semantics\n\nthat I-Sets provide domains for FD variables.\n\nGiven the two CLP languages LFD and L\nI-Set, we define the CLP language L\n\nas the union of the two languages, with a further constraint, :: , defined as follows:\n\n\u2022 the signature \u03A3 = \u03A3FD \u222A \u03A3I-Set \u222A { :: };\n\u2022 the intended interpretation D keeps the original mappings in the FD and\n\nI-Set sorts; i.e., D|\u03A3F D\n= DFD and D|\u03A3I-Set\n\n= DI-Set.\n\nThe declarative semantics of the constraint is\n\nX :: S \u2194 X \u2208 S\n\nwhere X is a FD variable. The :: /2 constraint links a FD variable to its domain\n\nas in most CLP(FD) frameworks, with the difference that S , being and I-Set,\n\nmay be non-completely specified. The :: /2 constraint should not be confused with\n\nthe s-member/2 constraint of Section 2.1, which represents the constraint of set\n\nmembership between a ground element and an I-Set.\n\n\n\nA CHR-based Implementation of Known Arc-Consistency 7\n\nFig. 1. FD Constraints as filters on variable domains\n\nIn CLP(FD) systems, domains provide ancillary information about variables. Do-\n\nmains contain the possible values that a variable can take; if a value is not consistent\n\nwith the imposed constraints, it is operationally removed from the domain. This\n\nhelps many systems (Dincbas et al. 1988; Puget 1994; IC-Parc 2001; SICStus 2003)\n\nto obtain higher performance; in fact, domain wipe-outs are detected early and\n\nmany alternatives are efficiently pruned. On the other hand, in the I-Set sort, do-\n\nmains must be manipulated as logical entities: if an element declaratively belongs\n\nto a domain, it cannot be removed."",""title"":""0408056v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0408/0408056v1.pdf""},{""chunk"":""Lloyd and Hill [HL94].\n\nA way to overcome the restriction in item 2. was proposed in 1987 by Jaffar\nand Lassez in their influential CLP(X) scheme that led to constraint logic pro-\ngramming. In this proposal the computation takes place over an arbitrary inter-\npretation and the queries and clause bodies can contain constraints, i.e., atomic\nformulas interpreted over the chosen interpretation. The unification mechanism\nis replaced by a more general process of constraint solving and the outcome of\na computation is a sequence of constraints to which the original query reduces.\n\nThis powerful idea was embodied since then in many constraint logic pro-\ngramming languages, starting with the CLP(R) language of Jaffar, Michaylov,\nStuckey, and Yap [JMSY92] in which linear constraints over reals were allowed,\nand the CHIP language of Dincbas et al. [DVS\u002B88] in which linear constraints\nover finite domains, combined with constraint propagation, were introduced. A\ntheoretical framework for CHIP was provided in van Hentenryck [Van89].\n\nThis transition from logic programming to constraint logic programming in-\ntroduced a new element. In the CLP(X) scheme the test for satisfiability of a se-\nquence of constraints was needed, while a proper account of the CHIP computing\nprocess required an introduction of constraint propagation into the framework.\nOn some interpretations these procedures can be undecidable (the satisfiability\ntest) or computationally expensive (the \u201Cideal\u201D constraint propagation). This\nexplains why in the realized implementations some approximation of the former\nor limited instances of the latter were chosen for.\n\nSo in both approaches the computation (i.e., the deduction) process needs\nto be parametrized by external procedures that for each specific interpretation\nhave to be provided and implemented separately. In short, in both cases the\ncomputation process, while parametrized by the considered interpretation, also\ndepends on the external procedures used."",""title"":""0005008v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0005/0005008v1.pdf""},{""chunk"":""A proof procedure for CLP is defined as an extension\nof standard resolution. A state is defined as a pair \u3008\u2190\na, A || C\u3009 of a goal and a set of constraints. At each\nstep of the computation, some literal a is selected from\nthe current goal according to some selection function.\nIf a is a constraint predicate then the next state is \u3008\u2190\nA || C \u2227 a\u3009 if solve(C \u2227 a) 6= false or \u3008\u2737 || false\u3009\notherwise. If a is a normal atom then the next state is\n\u3008\u2190 ~s = ~t, B, A || C\u3009 for some clause b \u2190 B where a is\n\nof the form p(~s) and b is of the form p(~t).\nA well suited constraint domain for representing\n\nCSPs is that of finite domain integer arithmetic\nCLP (FD) (Codognet \u0026 Diaz 1996; Van Hentenryck,\nSaraswat, \u0026 Deville 1998). It includes standard arith-\nmetic relations like =, 6=, \u003C and functions \u002B,\u2212, \u2217 with\ntheir usual interpretation. The implementation of\nCLP (FD) is based on consistency (Tsang 1993) algo-\nrithms.\n\nA CSP represented as a logic program can be trans-\nlated to a CLP(FD) program in a straightforward way.\nFirst, the domains of the variables are declared by a\nspecial CLP predicate (for example X in 1..n) and\nthe constraints are defined using the CLP predicates\nand functions. Then the execution of the goal \u2190\ncsp(X1, . . . , Xn) by a constraint proof procedure will\nresult in a set of constraints which are then solved effi-\nciently by consistency techniques.\n\nAn Alternative Representation\n\nFrom the examples and the discussion in the previous\nsection, the following general methodology can be given\nfor representing a parameterized CSP as a (constraint)\nlogic program. First, we create some data structure\nwhere each value of the parameter of the problem (e.g.\nthe number of queens, or the number of vertices) is as-\nsociated with a different variable. Then we define some\nrecursive predicates which iterate over this data struc-\nture and define constraints between the variables whose\ncorresponding parameters satisfy certain conditions."",""title"":""0003026v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0003/0003026v1.pdf""},{""chunk"":""for if we add the\nfollowing to our query\n\nminimum(set([R],(exists(W) : reserve(W,R)), M),\nmaximize(M).\n\nThis means that we are interested in a solution in which\nthe minimal reserve for one year is as high as possible.\n\nExcept for the representation of the optimization\nfunction, the above representation is very similar to the\none used in (Denecker et al. 1997; Seghers \u0026 Baeyens\n1996). The actual problem, given by the company, con-\nsists of scheduling 56 maintenances for 46 units in one\nyear. The original system needed for this 24 hours to\nsetup the CLP constraint store. The bottleneck was\nthe reduction of the aggregates. In the current imple-\nmentation of SLDNFA the construction only takes 45\nseconds. The huge difference comes from the fact that\nin the original system aggregates were implemented as\nlarge disjunctions, over which was backtracked until a\nconsistent constraint store was found. The current im-\nplementation will reduce the aggregates to a large fi-\nnite domain constraint at once, and will not backtrack\nwithin it. It is the CLP solver which backtracks in\nthe constructed constraint store; which is much faster\nthan the abductive solver can do. This difference also\nexplains why the current implementation spends more\ntime to find a good (optimal) solution (in 20 minutes\nwe find a solution which is 94% away from the optimal\nwhich is 2 or 3 times slower a the original one): the\ncurrent constructed constraint store will contain all the\ndisjunctions over which the original system had been\nbacktracking to construct a small one.\n\nA comparison with a pure CLP solution and the\nabove solution shows clearly the tradeoff between\ndeclarative representation and a very fast solution. The\npure (optimized) CLP solution will setup its constraint\nstore in several seconds (3 to 4 seconds), and find the\nsame solution as the above specification within 2 min-\nutes (compared to 20 minutes). But on the other hand\nthe CLP solution is a long program (400 lines) devel-"",""title"":""0003030v1.pdf"",""location"":""https://demodatasetsp.blob.core.windows.net/arxivcs/arxivcs/0003/0003030v1.pdf""}]"
